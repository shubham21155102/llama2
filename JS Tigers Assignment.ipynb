{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4298,"sourceType":"modelInstanceVersion","modelInstanceId":3093}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        ","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-17T07:55:58.960648Z","iopub.execute_input":"2024-04-17T07:55:58.960972Z","iopub.status.idle":"2024-04-17T07:55:58.974996Z","shell.execute_reply.started":"2024-04-17T07:55:58.960945Z","shell.execute_reply":"2024-04-17T07:55:58.973987Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/llama-2/pytorch/7b-chat-hf/1/model.safetensors.index.json\n/kaggle/input/llama-2/pytorch/7b-chat-hf/1/config.json\n/kaggle/input/llama-2/pytorch/7b-chat-hf/1/model-00001-of-00002.safetensors\n/kaggle/input/llama-2/pytorch/7b-chat-hf/1/model-00002-of-00002.safetensors\n/kaggle/input/llama-2/pytorch/7b-chat-hf/1/pytorch_model-00002-of-00002.bin\n/kaggle/input/llama-2/pytorch/7b-chat-hf/1/README.md\n/kaggle/input/llama-2/pytorch/7b-chat-hf/1/USE_POLICY.md\n/kaggle/input/llama-2/pytorch/7b-chat-hf/1/tokenizer.json\n/kaggle/input/llama-2/pytorch/7b-chat-hf/1/tokenizer_config.json\n/kaggle/input/llama-2/pytorch/7b-chat-hf/1/pytorch_model.bin.index.json\n/kaggle/input/llama-2/pytorch/7b-chat-hf/1/LICENSE.txt\n/kaggle/input/llama-2/pytorch/7b-chat-hf/1/pytorch_model-00001-of-00002.bin\n/kaggle/input/llama-2/pytorch/7b-chat-hf/1/special_tokens_map.json\n/kaggle/input/llama-2/pytorch/7b-chat-hf/1/.gitattributes\n/kaggle/input/llama-2/pytorch/7b-chat-hf/1/tokenizer.model\n/kaggle/input/llama-2/pytorch/7b-chat-hf/1/added_tokens.json\n/kaggle/input/llama-2/pytorch/7b-chat-hf/1/generation_config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM","metadata":{"execution":{"iopub.status.busy":"2024-04-17T07:55:58.976374Z","iopub.execute_input":"2024-04-17T07:55:58.976909Z","iopub.status.idle":"2024-04-17T07:56:05.010657Z","shell.execute_reply.started":"2024-04-17T07:55:58.976877Z","shell.execute_reply":"2024-04-17T07:56:05.009654Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"model = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\nprompt = \"What is the approximate number of luffas that a fully grown luffa tree can produce in a single growing season?\\n\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\nmodel = AutoModelForCausalLM.from_pretrained(model)\n\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    tokenizer=tokenizer,\n)\n\nsequences = pipeline(\n    prompt,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    max_length=200,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T07:56:05.011997Z","iopub.execute_input":"2024-04-17T07:56:05.012396Z","iopub.status.idle":"2024-04-17T08:02:11.365924Z","shell.execute_reply.started":"2024-04-17T07:56:05.012370Z","shell.execute_reply":"2024-04-17T08:02:11.364933Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1904c84eb084e4a9476cbddf0c358e4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n2024-04-17 07:58:30.034347: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-17 07:58:30.034453: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-17 07:58:30.172962: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"for seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-17T08:02:11.368175Z","iopub.execute_input":"2024-04-17T08:02:11.369056Z","iopub.status.idle":"2024-04-17T08:02:11.374112Z","shell.execute_reply.started":"2024-04-17T08:02:11.369020Z","shell.execute_reply":"2024-04-17T08:02:11.373219Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Result: What is the approximate number of luffas that a fully grown luffa tree can produce in a single growing season?\nThere are many factors that contribute to the number of luffas a luffa tree can produce in a single growing season. Here are some general guidelines:\n1. Luffa cultivars: Different varieties of luffa (Luffa aegyptiaca) produce varying numbers of fruit. 'Dwarf' or 'Compact' cultivars tend to produce fewer fruit than 'Standard' or 'Tree-Onager' cultivars.\n2. Growing conditions: The number of fruit produced by a luffa tree can be influenced by factors such as water, sunlight, soil quality, and fertilization. Better growing conditions (e.g., more sunlight, better soil) can lead to higher yields.\n3. Age of the tree: Mature luffa trees generally produce\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt=\"What should i do if i want to learn how to make unity game.\"\nsequences = pipeline(\n    prompt,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    max_length=200,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-17T08:02:11.375470Z","iopub.execute_input":"2024-04-17T08:02:11.375978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install fastapi uvicorn nest-asyncio pyngrok","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=['*'],\n    allow_credentials=True,\n    allow_methods=['*'], \n    allow_headers=['*'],\n)\n\n@app.get('/')\nasync def root():\n    return {'hello': 'world'}\n@app.post(\"/check\")\nasync def check_post(prompt: dict):\n    if \"prompt\" in prompt and isinstance(prompt[\"prompt\"], str):\n        prompt_value = prompt[\"prompt\"]\n#         prompt=\"What should i do if i want to learn how to make unity game.\"\n        sequences = pipeline(\n           prompt_value,\n           do_sample=True,\n           top_k=10,\n           num_return_sequences=1,\n           eos_token_id=tokenizer.eos_token_id,\n           max_length=200,\n              )\n        for seq in sequences:\n            print(f\"Result: {seq['generated_text']}\")\n    else:\n        return {\"error\": \"Invalid input format. 'prompt' field (string) is required.\"}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nest_asyncio\nfrom pyngrok import ngrok\nimport uvicorn\n\n\n\nport = 8000\nngrok.set_auth_token(\"\")\nngrok_tunnel = ngrok.connect(port)\n\n\nprint('Public URL:', ngrok_tunnel.public_url)\n\n\nnest_asyncio.apply()\n\nuvicorn.run(app, port=port)","metadata":{},"execution_count":null,"outputs":[]}]}